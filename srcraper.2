import requests
from bs4 import BeautifulSoup
import concurrent.futures

class FourChanScraper:
    def __init__(self):
        self.results = []

    def scrape_page(self, url):
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'lxml')
            for thread in soup.select('div.thread'):
                for post in thread.select('div.post'):
                    post_lower = post.text.lower()
                    if 'keyword_1' in post_lower:
                        post_id = post['id'][1:]
                        thread_id = thread['id'][1:]
                        board = url.split('/')[3]
                        url = f"https://boards.4chan.org/{board}/thread/{thread_id}#p{post_id}"
                        self.results.append(f"<a href='{url}'>{url}</a><br>")

        except Exception as e:
            print(f"Error scraping {url}: {e}")

    def run(self):
        # Set up a ThreadPoolExecutor with 8 worker threads
        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            # Generate URLs using a generator expression
            url_template = 'https://boards.4chan.org/{}/{}'
            boards = ['a', 'g', 'm', 'cgl', 'w', 'v', 'vg', 'vr', 'co', 'diy', 'fa', 'fit', 'int', 'jp', 'lit', 'mu', 'n', 'po', 'pol', 'sci', 'soc', 'sp', 't', 'tv', 'u', 'x']
            page_numbers = range(1, 11)
            urls = (url_template.format(board, page_number) for board in boards for page_number in page_numbers)

            # Submit scraping tasks for each URL
            futures = [executor.submit(self.scrape_page, url) for url in urls]
            # Wait for all tasks to complete
            concurrent.futures.wait(futures)

        # After all threads have finished, write results to the file:
        with open("output.html", "w") as f:
            for result in self.results:
                f.write(result)


if __name__ == "__main__":
    scraper = FourChanScraper()
    scraper.run()

